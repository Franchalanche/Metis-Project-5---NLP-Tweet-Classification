{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:1000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:1000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "df = pd.read_csv(\"Cleaned_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df6576b1a627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z:,@#'\\s]+\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%s]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"Tweet\"]=df.Tweet.apply(lambda x: re.sub(\"[^a-zA-Z:,@#'\\s]+\",\"\",x))\n",
    "df[\"Tweet\"]=df[\"Tweet\"].apply(lambda x: x.lower())\n",
    "df[\"Tweet\"] = df.Tweet.map(lambda x: re.sub('[%s]' % re.escape(string.punctuation),' ',x))\n",
    "stemmer = LancasterStemmer()\n",
    "df[\"Tweet\"] = df[\"Tweet\"].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"] = df[\"Tweet\"].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_400K = df[\"Tweet\"][:400_001]\n",
    "tweets_400K_list = list(tweets_400K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_001 = CountVectorizer(stop_words = 'english', min_df = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaepiccorelli/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_400 = cv_001.fit_transform(tweets_400K_list)\n",
    "df_400 = pd.DataFrame(X_400.toarray(),columns=cv_001.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_400 = list(df_400.columns)\n",
    "#col_list_400[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaepiccorelli/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "words_400_001 = np.array(cv_001.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmf_400' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-668cd8e7e80a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnmf_400_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW_400_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_400\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mH_400_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_400\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_400_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nmf_400' is not defined"
     ]
    }
   ],
   "source": [
    "nmf_400_10 = NMF(n_components=10, solver=\"mu\")\n",
    "W_400_10 = nmf_400.fit_transform(X_400)\n",
    "H_400_10 = nmf_400.components_\n",
    "\n",
    "for i, topic in enumerate(H_400_10):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words_400[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_01 = CountVectorizer(stop_words = 'english', min_df = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_400_01 = cv_01.fit_transform(tweets_400K_list)\n",
    "df_400_01 = pd.DataFrame(X_400_01.toarray(),columns=cv_01.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_400_01 = np.array(cv_01.get_feature_names())\n",
    "nmf_400_01 = NMF(n_components=20, solver=\"mu\")\n",
    "W_400_01 = nmf_400_01.fit_transform(X_400_01)\n",
    "H_400_01 = nmf_400_01.components_\n",
    "\n",
    "for i, topic in enumerate(H_400_01):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words_400_01[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "addl_stop_words = [\"anymore\",\n",
    "\"bit\",\n",
    "\"ago\",\n",
    "\"absolutely\",\n",
    "\"bitch\",\n",
    "\"aw\",\n",
    "\"aww\",\n",
    "\"awww\",\n",
    "\"awwww\",\n",
    "\"alot\",\n",
    "\"babe\",\n",
    "\"ah\",\n",
    "\"ahh\",\n",
    "\"ahhh\",\n",
    "\"blah\",\n",
    "\"bc\",\n",
    "\"ass\",\n",
    "\"argh\",\n",
    "\"apparently\",\n",
    "\"aint\",\n",
    "\"ahead\",\n",
    "\"lol\",\n",
    "\"did\",\n",
    "\"do\",\n",
    "\"oh\",\n",
    "\"come\",\n",
    "\"http\",\n",
    "\"didn\",\n",
    "\"ve\",\n",
    "\"ll\",\n",
    "\"dont\",\n",
    "\"gonna\",\n",
    "\"going\",\n",
    "\"go\",\n",
    "\"don\",\n",
    "\"long\",\n",
    "\"soon\",\n",
    "\"tonight\",\n",
    "\"tomorrow\",\n",
    "\"just\",\n",
    "\"today\",\n",
    "\"really\",\n",
    "\"days\",\n",
    "\"hours\",\n",
    "\"week\",\n",
    "\"weekend\",\n",
    "\"getting\",\n",
    "\"get\",\n",
    "\"got\",\n",
    "\"love\",\n",
    "\"wish\",\n",
    "\"haha\",\n",
    "\"great\",\n",
    "\"little\",\n",
    "\"yeah\",\n",
    "\"way\",\n",
    "\"make\",\n",
    "\"new\",\n",
    "\"weeks\",\n",
    "\"day\",\n",
    "\"day\",\n",
    "\"ugh\",\n",
    "\"wanna\",\n",
    "\"tired\",\n",
    "\"morning\",\n",
    "\"people\",\n",
    "\"feeling\",\n",
    "\"like\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stop_word_list = text.ENGLISH_STOP_WORDS.union(addl_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_01_sw = CountVectorizer(stop_words = stop_word_list, min_df = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_400_01_sw = cv_01_sw.fit_transform(tweets_400K_list)\n",
    "df_400_01_sw = pd.DataFrame(X_400_01_sw.toarray(),columns=cv_01_sw.get_feature_names())\n",
    "words_400_01_sw = np.array(cv_01_sw.get_feature_names())\n",
    "nmf_400_01_sw = NMF(n_components=20, solver=\"mu\")\n",
    "W_400_01_sw = nmf_400_01_sw.fit_transform(X_400_01_sw)\n",
    "H_400_01_sw = nmf_400_01_sw.components_\n",
    "\n",
    "for i, topic in enumerate(H_400_01_sw):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words_400_01_sw[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_printer(cv,topic_count,solver_type,tweet_list):\n",
    "    X = cv.fit_transform(tweet_list)\n",
    "    df = pd.DataFrame(X.toarray(),columns=cv.get_feature_names())\n",
    "    words = np.array(cv.get_feature_names())\n",
    "    nmf = NMF(n_components=topic_count,solver=solver_type)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    for i, topic in enumerate(H):\n",
    "        print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_01_sw_max = CountVectorizer(stop_words = stop_word_list, min_df = .01,max_df=.75)\n",
    "topic_count_wmx=20\n",
    "solver=\"mu\"\n",
    "tweet_list=tweets_400K_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_printer(cv_01_sw_max,topic_count_wmx,solver,tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_types = [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_only(string_list):\n",
    "    new_tweet_list = []\n",
    "    for string in string_list:\n",
    "        new_string_list = []\n",
    "        #word_list = string.split(\" \")\n",
    "        #print(word_list)\n",
    "        word_pos_list = pos_tag(string.split(\" \"))\n",
    "        for item in word_pos_list:\n",
    "            if item[1] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]:\n",
    "                new_string_list.append(item[0])\n",
    "        new_string = \" \".join(new_string_list)\n",
    "        new_tweet_list.append(new_string)\n",
    "    return new_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_string_list = [\"Victoria is going to Washington, DC to see George Washington's monument.\",\n",
    "                    \"Jane fell down the well and hit water\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns_only_list = nouns_only(test_string_list)\n",
    "nouns_only_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
