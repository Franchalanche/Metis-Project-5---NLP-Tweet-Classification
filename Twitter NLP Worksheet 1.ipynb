{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Cleaned_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@kenichan i dived many times for the ball mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no it's not behaving at all i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date_Time       Username  \\\n",
       "0  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
       "1  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
       "2  Mon Apr 06 22:19:57 PDT 2009        ElleCTF   \n",
       "3  Mon Apr 06 22:19:57 PDT 2009         Karoli   \n",
       "4  Mon Apr 06 22:20:00 PDT 2009       joy_wolf   \n",
       "\n",
       "                                               Tweet  \n",
       "0  is upset that he can't update his facebook by ...  \n",
       "1  @kenichan i dived many times for the ball mana...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no it's not behaving at all i...  \n",
       "4                      @kwesidei not the whole crew   "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special characters & digits aside from @, #, apostrophes (for contractions) and spaces\n",
    "df[\"Tweet\"]=df.Tweet.apply(lambda x: re.sub(\"[^a-zA-Z:,@#'\\s]+\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date_Time       Username  \\\n",
       "0  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
       "1  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
       "2  Mon Apr 06 22:19:57 PDT 2009        ElleCTF   \n",
       "3  Mon Apr 06 22:19:57 PDT 2009         Karoli   \n",
       "4  Mon Apr 06 22:20:00 PDT 2009       joy_wolf   \n",
       "\n",
       "                                               Tweet  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball Mana...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all ...  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers got removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"]=df.Tweet.apply(lambda x: re.sub(\",\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no it's not behaving at all i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date_Time       Username  \\\n",
       "0  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
       "1  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
       "2  Mon Apr 06 22:19:57 PDT 2009        ElleCTF   \n",
       "3  Mon Apr 06 22:19:57 PDT 2009         Karoli   \n",
       "4  Mon Apr 06 22:20:00 PDT 2009       joy_wolf   \n",
       "\n",
       "                                               Tweet  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball Mana...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no it's not behaving at all i...  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###apply(lambda x: re.sub(\".\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the underscores\n",
    "df[\"Tweet\"]=df.Tweet.apply(lambda x: re.sub(\"_+\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"]=df[\"Tweet\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@kenichan i dived many times for the ball mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no it's not behaving at all i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date_Time       Username  \\\n",
       "0  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
       "1  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
       "2  Mon Apr 06 22:19:57 PDT 2009        ElleCTF   \n",
       "3  Mon Apr 06 22:19:57 PDT 2009         Karoli   \n",
       "4  Mon Apr 06 22:20:00 PDT 2009       joy_wolf   \n",
       "\n",
       "                                               Tweet  \n",
       "0  is upset that he can't update his facebook by ...  \n",
       "1  @kenichan i dived many times for the ball mana...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no it's not behaving at all i...  \n",
       "4                      @kwesidei not the whole crew   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @thenspcc @sparkscharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date_Time         Username  \\\n",
       "1599994  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
       "1599995  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
       "1599996  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
       "1599997  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
       "1599998  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
       "\n",
       "                                                     Tweet  \n",
       "1599994  just woke up having no school is the best feel...  \n",
       "1599995  thewdbcom  very cool to hear old walt intervie...  \n",
       "1599996  are you ready for your mojo makeover ask me fo...  \n",
       "1599997  happy th birthday to my boo of alll time tupac...  \n",
       "1599998  happy #charitytuesday @thenspcc @sparkscharity...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to remove question marks, periods without using REGEX shortwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Tweet.apply(lambda x: re.sub(\"?\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_mass = tweets_massaged.map(lambda x: re.sub('[%s]' % re.escape(string.punctuation),'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to remove special characters\n",
    "#re.findall(\"^((?!\\w).)*$\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Remove punctuation? Bi-grams & tri-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Make a matrix using CV, and also one using TF-IDF. then use cosine similarity to see which topics are most similar\n",
    "# Redo matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### text processing in NLTK. LancasterStemmer()\n",
    "### filter speech to include adjectives and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vader sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Topic modelling techniques - LSA & LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english', min_df = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df[\"Tweet\"]\n",
    "#tweets_list = list(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_mass = tweets.apply(lambda x: re.sub(\",\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_mass_2 = tweets_mass.apply(lambda x: re.sub(\".\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_mass_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_massaged = tweets_mass.apply(lambda x: re.sub(\"_+\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_100K = df[\"Tweet\"][:100001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_100K_list = list(tweets_100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mass = cv.fit_transform(tweets_100K_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaepiccorelli/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "term_df=pd.DataFrame(X_mass.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = list(term_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolutely', 'account', 'ache', 'actually', 'adam', 'add', 'afford', 'afraid', 'afternoon', 'ago', 'agree', 'ah', 'ahead', 'ahh', 'ahhh', 'aint', 'air', 'airport', 'alarm', 'allergies', 'alot', 'amazing', 'amp', 'annoying', 'answer', 'anymore', 'app', 'apparently', 'april', 'aren', 'argh', 'art', 'ask', 'asked', 'asleep', 'asot', 'ass', 'assignment', 'ate', 'available', 'aw', 'awake', 'away', 'awesome', 'awful', 'aww', 'awww', 'awwww', 'baby', 'bad', 'badly', 'bamboozle', 'band', 'bank', 'bar', 'barely', 'battery', 'bb', 'bbq', 'bc', 'bday', 'beach', 'beat', 'beautiful', 'bed', 'beer', 'believe', 'best', 'bet', 'better', 'bf', 'big', 'bike', 'birthday', 'bit', 'bitch', 'black', 'blah', 'blog', 'bloody', 'blue', 'body', 'boo', 'book', 'books', 'bored', 'boring', 'bought', 'bout', 'box', 'boy', 'boyfriend', 'boys', 'brain', 'break', 'breakfast', 'bring', 'broke', 'broken', 'brother', 'btw', 'bummed', 'bummer', 'burnt', 'bus', 'business', 'busy', 'buy', 'bye', 'cake', 'called', 'came', 'camera', 'cancelled', 'car', 'card', 'care', 'case', 'cat', 'catch', 'caught', 'cause', 'chance', 'change', 'changed', 'chat', 'check', 'chocolate', 'church', 'city', 'class', 'classes', 'clean', 'cleaning', 'close', 'closed', 'clothes', 'club', 'coffee', 'cold', 'college', 'come', 'comes', 'coming', 'company', 'completely', 'computer', 'concert', 'confused', 'cook', 'cool', 'cos', 'cough', 'couldn', 'couple', 'course', 'coz', 'crap', 'crappy', 'crazy', 'cream', 'cried', 'crying', 'cup', 'cut', 'cute', 'cuz', 'da', 'dad', 'dammit', 'damn', 'dance', 'dang', 'danny', 'dark', 'darn', 'date', 'daughter', 'david', 'day', 'days', 'ddlovato', 'dead', 'dear', 'death', 'decided', 'definitely', 'dentist', 'depressed', 'depressing', 'did', 'didn', 'didnt', 'die', 'died', 'different', 'dinner', 'disappointed', 'doctor', 'does', 'doesn', 'doesnt', 'dog', 'doing', 'don', 'dont', 'door', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drive', 'driving', 'dropped', 'drunk', 'dude', 'dunno', 'dvd', 'dying', 'ear', 'earlier', 'early', 'easter', 'easy', 'eat', 'eating', 'em', 'email', 'end', 'ended', 'english', 'enjoy', 'enjoying', 'episode', 'especially', 'essay', 'evening', 'exactly', 'exam', 'exams', 'excited', 'exhausted', 'expensive', 'eye', 'eyes', 'face', 'facebook', 'fact', 'fail', 'failed', 'fair', 'fall', 'falling', 'family', 'fan', 'far', 'fast', 'fat', 'favorite', 'fb', 'feel', 'feelin', 'feeling', 'feels', 'feet', 'fell', 'felt', 'fever', 'fight', 'figure', 'film', 'final', 'finally', 'finals', 'fine', 'finish', 'finished', 'fit', 'fix', 'fixed', 'flight', 'flu', 'fml', 'follow', 'followers', 'following', 'food', 'foot', 'football', 'forever', 'forget', 'forgot', 'forward', 'freaking', 'free', 'french', 'friday', 'friend', 'friends', 'fuck', 'fucking', 'fun', 'funny', 'gah', 'game', 'games', 'garden', 'gave', 'gay', 'gets', 'gettin', 'getting', 'girl', 'girls', 'giving', 'glad', 'god', 'goes', 'goin', 'going', 'gone', 'gonna', 'good', 'goodbye', 'goodnight', 'google', 'gosh', 'got', 'gotta', 'great', 'green', 'gt', 'guess', 'gutted', 'guy', 'guys', 'gym', 'ha', 'haha', 'hahaha', 'hair', 'half', 'hand', 'hands', 'hang', 'hanging', 'hangover', 'happen', 'happened', 'happens', 'happy', 'hard', 'hasn', 'hate', 'hates', 'haven', 'havent', 'having', 'head', 'headache', 'heading', 'hear', 'heard', 'heart', 'hehe', 'hell', 'hello', 'help', 'hey', 'hi', 'high', 'history', 'hit', 'hmm', 'hmmm', 'hold', 'holiday', 'holidays', 'home', 'homework', 'hope', 'hopefully', 'hoping', 'horrible', 'hospital', 'hot', 'hotel', 'hour', 'hours', 'house', 'hrs', 'http', 'hubby', 'hug', 'huge', 'hugs', 'hungry', 'hurt', 'hurting', 'hurts', 'ice', 'idea', 'idk', 'ill', 'im', 'inside', 'instead', 'interesting', 'internet', 'iphone', 'ipod', 'isn', 'isnt', 'issues', 'itunes', 'ive', 'jealous', 'job', 'jonas', 'june', 'jus', 'just', 'keeps', 'kid', 'kids', 'kill', 'killed', 'killing', 'kind', 'kinda', 'kitty', 'knee', 'knew', 'know', 'knows', 'la', 'lady', 'lame', 'laptop', 'late', 'lately', 'later', 'laundry', 'laying', 'lazy', 'learn', 'leave', 'leaving', 'left', 'leg', 'legs', 'let', 'library', 'life', 'like', 'lil', 'line', 'link', 'list', 'listen', 'listening', 'little', 'live', 'living', 'll', 'lmao', 'load', 'loads', 'lol', 'london', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lose', 'losing', 'loss', 'lost', 'lot', 'lots', 'love', 'loved', 'lovely', 'loves', 'low', 'lt', 'luck', 'lucky', 'lunch', 'mac', 'mad', 'make', 'makes', 'making', 'man', 'math', 'matter', 'maybe', 'mcfly', 'mean', 'means', 'meant', 'meet', 'meeting', 'message', 'messed', 'middle', 'miles', 'mileycyrus', 'milk', 'min', 'mind', 'mins', 'minute', 'minutes', 'miss', 'missed', 'misses', 'missing', 'mobile', 'mom', 'moment', 'mommy', 'monday', 'mondays', 'money', 'month', 'months', 'mood', 'morning', 'mother', 'mothers', 'movie', 'movies', 'moving', 'mr', 'mum', 'music', 'myspace', 'nap', 'nd', 'near', 'nearly', 'neck', 'need', 'needed', 'needs', 'new', 'news', 'nice', 'night', 'nights', 'nite', 'nope', 'nose', 'number', 'office', 'officially', 'oh', 'ok', 'okay', 'old', 'omg', 'ones', 'online', 'open', 'order', 'ouch', 'outside', 'packing', 'page', 'pain', 'paper', 'parents', 'park', 'party', 'pass', 'passed', 'past', 'pay', 'pc', 'people', 'perfect', 'person', 'phone', 'photo', 'photos', 'pic', 'pick', 'pics', 'picture', 'pictures', 'pissed', 'pizza', 'place', 'plan', 'plans', 'play', 'played', 'playing', 'plus', 'pm', 'point', 'poor', 'post', 'power', 'ppl', 'pretty', 'probably', 'problem', 'problems', 'project', 'prom', 'ps', 'puppy', 'quiet', 'quite', 'quot', 'race', 'radio', 'rain', 'raining', 'rainy', 'ran', 'rd', 'read', 'reading', 'ready', 'real', 'realized', 'really', 'reason', 'red', 'remember', 'reply', 'rest', 'revising', 'revision', 'ride', 'right', 'rip', 'road', 'rock', 'room', 'round', 'ruined', 'run', 'running', 'sad', 'sadly', 'said', 'sat', 'saturday', 'save', 'saw', 'say', 'saying', 'says', 'scared', 'scary', 'school', 'screen', 'season', 'second', 'seeing', 'seen', 'send', 'sent', 'series', 'seriously', 'service', 'set', 'shall', 'shame', 'shift', 'shit', 'shoes', 'shoot', 'shop', 'shopping', 'short', 'shouldn', 'shower', 'shows', 'shut', 'sick', 'sigh', 'single', 'sister', 'sit', 'site', 'sitting', 'sleep', 'sleeping', 'sleepy', 'slept', 'slow', 'small', 'snow', 'sold', 'son', 'song', 'songs', 'soo', 'soon', 'sooo', 'soooo', 'sooooo', 'sore', 'sorry', 'sound', 'sounds', 'spend', 'spending', 'spent', 'spring', 'st', 'stand', 'star', 'start', 'started', 'starting', 'starts', 'stay', 'stayed', 'staying', 'stomach', 'stop', 'stopped', 'store', 'story', 'stuck', 'study', 'studying', 'stuff', 'stupid', 'suck', 'sucks', 'summer', 'sun', 'sunday', 'sunny', 'sunshine', 'super', 'support', 'suppose', 'supposed', 'sure', 'sweet', 'swine', 'taken', 'takes', 'taking', 'talk', 'talking', 'tea', 'team', 'teeth', 'tell', 'terrible', 'test', 'text', 'th', 'thank', 'thanks', 'thats', 'theres', 'thing', 'things', 'think', 'thinking', 'thinks', 'tho', 'thought', 'throat', 'thursday', 'ticket', 'tickets', 'til', 'till', 'time', 'times', 'tired', 'today', 'told', 'tom', 'tommcfly', 'tomorrow', 'tonight', 'took', 'totally', 'tour', 'town', 'traffic', 'train', 'trek', 'tried', 'trip', 'trouble', 'true', 'try', 'trying', 'tuesday', 'tummy', 'turn', 'turned', 'tv', 'tweet', 'tweetdeck', 'tweeting', 'tweets', 'twitter', 'ugh', 'uk', 'understand', 'unfortunately', 'uni', 'update', 'updates', 'upload', 'upset', 'ur', 'use', 'used', 'using', 'usual', 'usually', 'vacation', 've', 'vegas', 'video', 'visit', 'voice', 'wait', 'waiting', 'wake', 'waking', 'walk', 'walking', 'wanna', 'want', 'wanted', 'wanting', 'wants', 'warm', 'wasn', 'wasnt', 'watch', 'watched', 'watching', 'water', 'way', 'wear', 'wearing', 'weather', 'web', 'website', 'wedding', 'wednesday', 'week', 'weekend', 'weekends', 'weeks', 'weird', 'went', 'wet', 'whats', 'white', 'wife', 'win', 'window', 'windows', 'wish', 'wishes', 'wishing', 'wit', 'woke', 'wolverine', 'won', 'wonder', 'wondering', 'wont', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worried', 'worse', 'worst', 'worth', 'wouldn', 'wow', 'write', 'writing', 'wrong', 'wtf', 'xd', 'xx', 'xxx', 'ya', 'yay', 'yea', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yo', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def need stemming & lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_100K_stem = tweets_100K.apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaepiccorelli/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tweets_100K_stem_list = list(tweets_100K_stem)\n",
    "X_mass_stem = cv.fit_transform(tweets_100K_stem_list)\n",
    "term_stem_df=pd.DataFrame(X_mass_stem.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolutely', 'account', 'ache', 'actually', 'adam', 'add', 'afford', 'afraid', 'afternoon', 'ago', 'agree', 'ah', 'ahead', 'ahh', 'ahhh', 'aint', 'air', 'airport', 'alarm', 'allergies', 'alot', 'amazing', 'amp', 'annoying', 'answer', 'anymore', 'app', 'apparently', 'april', 'aren', 'argh', 'art', 'ask', 'asked', 'asleep', 'asot', 'ass', 'assignment', 'ate', 'available', 'aw', 'awake', 'away', 'awesome', 'awful', 'aww', 'awww', 'awwww', 'baby', 'bad', 'badly', 'bamboozle', 'band', 'bank', 'bar', 'barely', 'battery', 'bb', 'bbq', 'bc', 'bday', 'beach', 'beat', 'beautiful', 'bed', 'beer', 'believe', 'best', 'bet', 'better', 'bf', 'big', 'bike', 'birthday', 'bit', 'bitch', 'black', 'blah', 'blog', 'bloody', 'blue', 'body', 'boo', 'book', 'books', 'bored', 'boring', 'bought', 'bout', 'box', 'boy', 'boyfriend', 'boys', 'brain', 'break', 'breakfast', 'bring', 'broke', 'broken', 'brother', 'btw', 'bummed', 'bummer', 'burnt', 'bus', 'business', 'busy', 'buy', 'bye', 'cake', 'called', 'came', 'camera', 'cancelled', 'car', 'card', 'care', 'case', 'cat', 'catch', 'caught', 'cause', 'chance', 'change', 'changed', 'chat', 'check', 'chocolate', 'church', 'city', 'class', 'classes', 'clean', 'cleaning', 'close', 'closed', 'clothes', 'club', 'coffee', 'cold', 'college', 'come', 'comes', 'coming', 'company', 'completely', 'computer', 'concert', 'confused', 'cook', 'cool', 'cos', 'cough', 'couldn', 'couple', 'course', 'coz', 'crap', 'crappy', 'crazy', 'cream', 'cried', 'crying', 'cup', 'cut', 'cute', 'cuz', 'da', 'dad', 'dammit', 'damn', 'dance', 'dang', 'danny', 'dark', 'darn', 'date', 'daughter', 'david', 'day', 'days', 'ddlovato', 'dead', 'dear', 'death', 'decided', 'definitely', 'dentist', 'depressed', 'depressing', 'did', 'didn', 'didnt', 'die', 'died', 'different', 'dinner', 'disappointed', 'doctor', 'does', 'doesn', 'doesnt', 'dog', 'doing', 'don', 'dont', 'door', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drive', 'driving', 'dropped', 'drunk', 'dude', 'dunno', 'dvd', 'dying', 'ear', 'earlier', 'early', 'easter', 'easy', 'eat', 'eating', 'em', 'email', 'end', 'ended', 'english', 'enjoy', 'enjoying', 'episode', 'especially', 'essay', 'evening', 'exactly', 'exam', 'exams', 'excited', 'exhausted', 'expensive', 'eye', 'eyes', 'face', 'facebook', 'fact', 'fail', 'failed', 'fair', 'fall', 'falling', 'family', 'fan', 'far', 'fast', 'fat', 'favorite', 'fb', 'feel', 'feelin', 'feeling', 'feels', 'feet', 'fell', 'felt', 'fever', 'fight', 'figure', 'film', 'final', 'finally', 'finals', 'fine', 'finish', 'finished', 'fit', 'fix', 'fixed', 'flight', 'flu', 'fml', 'follow', 'followers', 'following', 'food', 'foot', 'football', 'forever', 'forget', 'forgot', 'forward', 'freaking', 'free', 'french', 'friday', 'friend', 'friends', 'fuck', 'fucking', 'fun', 'funny', 'gah', 'game', 'games', 'garden', 'gave', 'gay', 'gets', 'gettin', 'getting', 'girl', 'girls', 'giving', 'glad', 'god', 'goes', 'goin', 'going', 'gone', 'gonna', 'good', 'goodbye', 'goodnight', 'google', 'gosh', 'got', 'gotta', 'great', 'green', 'gt', 'guess', 'gutted', 'guy', 'guys', 'gym', 'ha', 'haha', 'hahaha', 'hair', 'half', 'hand', 'hands', 'hang', 'hanging', 'hangover', 'happen', 'happened', 'happens', 'happy', 'hard', 'hasn', 'hate', 'hates', 'haven', 'havent', 'having', 'head', 'headache', 'heading', 'hear', 'heard', 'heart', 'hehe', 'hell', 'hello', 'help', 'hey', 'hi', 'high', 'history', 'hit', 'hmm', 'hmmm', 'hold', 'holiday', 'holidays', 'home', 'homework', 'hope', 'hopefully', 'hoping', 'horrible', 'hospital', 'hot', 'hotel', 'hour', 'hours', 'house', 'hrs', 'http', 'hubby', 'hug', 'huge', 'hugs', 'hungry', 'hurt', 'hurting', 'hurts', 'ice', 'idea', 'idk', 'ill', 'im', 'inside', 'instead', 'interesting', 'internet', 'iphone', 'ipod', 'isn', 'isnt', 'issues', 'itunes', 'ive', 'jealous', 'job', 'jonas', 'june', 'jus', 'just', 'keeps', 'kid', 'kids', 'kill', 'killed', 'killing', 'kind', 'kinda', 'kitty', 'knee', 'knew', 'know', 'knows', 'la', 'lady', 'lame', 'laptop', 'late', 'lately', 'later', 'laundry', 'laying', 'lazy', 'learn', 'leave', 'leaving', 'left', 'leg', 'legs', 'let', 'library', 'life', 'like', 'lil', 'line', 'link', 'list', 'listen', 'listening', 'little', 'live', 'living', 'll', 'lmao', 'load', 'loads', 'lol', 'london', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lose', 'losing', 'loss', 'lost', 'lot', 'lots', 'love', 'loved', 'lovely', 'loves', 'low', 'lt', 'luck', 'lucky', 'lunch', 'mac', 'mad', 'make', 'makes', 'making', 'man', 'math', 'matter', 'maybe', 'mcfly', 'mean', 'means', 'meant', 'meet', 'meeting', 'message', 'messed', 'middle', 'miles', 'mileycyrus', 'milk', 'min', 'mind', 'mins', 'minute', 'minutes', 'miss', 'missed', 'misses', 'missing', 'mobile', 'mom', 'moment', 'mommy', 'monday', 'mondays', 'money', 'month', 'months', 'mood', 'morning', 'mother', 'mothers', 'movie', 'movies', 'moving', 'mr', 'mum', 'music', 'myspace', 'nap', 'nd', 'near', 'nearly', 'neck', 'need', 'needed', 'needs', 'new', 'news', 'nice', 'night', 'nights', 'nite', 'nope', 'nose', 'number', 'office', 'officially', 'oh', 'ok', 'okay', 'old', 'omg', 'ones', 'online', 'open', 'order', 'ouch', 'outside', 'packing', 'page', 'pain', 'paper', 'parents', 'park', 'party', 'pass', 'passed', 'past', 'pay', 'pc', 'people', 'perfect', 'person', 'phone', 'photo', 'photos', 'pic', 'pick', 'pics', 'picture', 'pictures', 'pissed', 'pizza', 'place', 'plan', 'plans', 'play', 'played', 'playing', 'plus', 'pm', 'point', 'poor', 'post', 'power', 'ppl', 'pretty', 'probably', 'problem', 'problems', 'project', 'prom', 'ps', 'puppy', 'quiet', 'quite', 'quot', 'race', 'radio', 'rain', 'raining', 'rainy', 'ran', 'rd', 'read', 'reading', 'ready', 'real', 'realized', 'really', 'reason', 'red', 'remember', 'reply', 'rest', 'revising', 'revision', 'ride', 'right', 'rip', 'road', 'rock', 'room', 'round', 'ruined', 'run', 'running', 'sad', 'sadly', 'said', 'sat', 'saturday', 'save', 'saw', 'say', 'saying', 'says', 'scared', 'scary', 'school', 'screen', 'season', 'second', 'seeing', 'seen', 'send', 'sent', 'series', 'seriously', 'service', 'set', 'shall', 'shame', 'shift', 'shit', 'shoes', 'shoot', 'shop', 'shopping', 'short', 'shouldn', 'shower', 'shows', 'shut', 'sick', 'sigh', 'single', 'sister', 'sit', 'site', 'sitting', 'sleep', 'sleeping', 'sleepy', 'slept', 'slow', 'small', 'snow', 'sold', 'son', 'song', 'songs', 'soo', 'soon', 'sooo', 'soooo', 'sooooo', 'sore', 'sorry', 'sound', 'sounds', 'spend', 'spending', 'spent', 'spring', 'st', 'stand', 'star', 'start', 'started', 'starting', 'starts', 'stay', 'stayed', 'staying', 'stomach', 'stop', 'stopped', 'store', 'story', 'stuck', 'study', 'studying', 'stuff', 'stupid', 'suck', 'sucks', 'summer', 'sun', 'sunday', 'sunny', 'sunshine', 'super', 'support', 'suppose', 'supposed', 'sure', 'sweet', 'swine', 'taken', 'takes', 'taking', 'talk', 'talking', 'tea', 'team', 'teeth', 'tell', 'terrible', 'test', 'text', 'th', 'thank', 'thanks', 'thats', 'theres', 'thing', 'things', 'think', 'thinking', 'thinks', 'tho', 'thought', 'throat', 'thursday', 'ticket', 'tickets', 'til', 'till', 'time', 'times', 'tired', 'today', 'told', 'tom', 'tommcfly', 'tomorrow', 'tonight', 'took', 'totally', 'tour', 'town', 'traffic', 'train', 'trek', 'tried', 'trip', 'trouble', 'true', 'try', 'trying', 'tuesday', 'tummy', 'turn', 'turned', 'tv', 'tweet', 'tweetdeck', 'tweeting', 'tweets', 'twitter', 'ugh', 'uk', 'understand', 'unfortunately', 'uni', 'update', 'updates', 'upload', 'upset', 'ur', 'use', 'used', 'using', 'usual', 'usually', 'vacation', 've', 'vegas', 'video', 'visit', 'voice', 'wait', 'waiting', 'wake', 'waking', 'walk', 'walking', 'wanna', 'want', 'wanted', 'wanting', 'wants', 'warm', 'wasn', 'wasnt', 'watch', 'watched', 'watching', 'water', 'way', 'wear', 'wearing', 'weather', 'web', 'website', 'wedding', 'wednesday', 'week', 'weekend', 'weekends', 'weeks', 'weird', 'went', 'wet', 'whats', 'white', 'wife', 'win', 'window', 'windows', 'wish', 'wishes', 'wishing', 'wit', 'woke', 'wolverine', 'won', 'wonder', 'wondering', 'wont', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worried', 'worse', 'worst', 'worth', 'wouldn', 'wow', 'write', 'writing', 'wrong', 'wtf', 'xd', 'xx', 'xxx', 'ya', 'yay', 'yea', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yo', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "stem_column_list = list(term_stem_df.columns)\n",
    "print(stem_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will now apply porter stemmer to see if that works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_100K_stem_ps = tweets_100K_stem.apply(lambda x: ps.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_100K_stem_ps_list = list(tweets_100K_stem_ps)\n",
    "X_mass_stem_ps = cv.fit_transform(tweets_100K_stem_ps_list)\n",
    "term_stem_ps_df=pd.DataFrame(X_mass_stem_ps.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolutely', 'account', 'ache', 'actually', 'adam', 'add', 'afford', 'afraid', 'afternoon', 'ago', 'agree', 'ah', 'ahead', 'ahh', 'ahhh', 'aint', 'air', 'airport', 'al', 'alarm', 'allergies', 'alot', 'amazing', 'amp', 'annoying', 'answer', 'anymore', 'app', 'apparently', 'april', 'aren', 'argh', 'art', 'ask', 'asked', 'asleep', 'asot', 'ass', 'assignment', 'ate', 'available', 'aw', 'awake', 'away', 'awesome', 'awful', 'aww', 'awww', 'awwww', 'baby', 'bad', 'badly', 'bamboozle', 'band', 'bank', 'bar', 'barely', 'battery', 'bb', 'bbq', 'bc', 'bday', 'beach', 'beat', 'beautiful', 'bed', 'beer', 'believe', 'best', 'bet', 'bett', 'better', 'bf', 'big', 'bike', 'birthday', 'bit', 'bitch', 'black', 'blah', 'blog', 'bloody', 'blue', 'body', 'boo', 'book', 'books', 'bored', 'boring', 'bought', 'bout', 'box', 'boy', 'boyfriend', 'boys', 'brain', 'break', 'breakfast', 'bring', 'broke', 'broken', 'brother', 'btw', 'bummed', 'bummer', 'burnt', 'bus', 'business', 'busy', 'buy', 'bye', 'cake', 'called', 'came', 'camera', 'cancelled', 'car', 'card', 'care', 'case', 'cat', 'catch', 'caught', 'cause', 'chance', 'change', 'changed', 'chat', 'check', 'chocolate', 'church', 'city', 'class', 'classes', 'clean', 'cleaning', 'close', 'closed', 'clothes', 'club', 'coffee', 'cold', 'college', 'come', 'comes', 'coming', 'company', 'completely', 'computer', 'concert', 'confused', 'cook', 'cool', 'cos', 'cough', 'couldn', 'couple', 'course', 'coz', 'crap', 'crappy', 'crazy', 'cream', 'cried', 'crying', 'cup', 'cut', 'cute', 'cuz', 'da', 'dad', 'dammit', 'damn', 'dance', 'dang', 'danny', 'dark', 'darn', 'date', 'daughter', 'david', 'day', 'days', 'ddlovato', 'dead', 'dear', 'death', 'decided', 'definitely', 'dentist', 'depressed', 'depressing', 'did', 'didn', 'didnt', 'die', 'died', 'different', 'dinner', 'disappointed', 'doctor', 'does', 'doesn', 'doesnt', 'dog', 'doing', 'don', 'dont', 'door', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drive', 'driving', 'dropped', 'drunk', 'dude', 'dunno', 'dvd', 'dying', 'ear', 'earlier', 'early', 'easter', 'easy', 'eat', 'eating', 'em', 'email', 'end', 'ended', 'english', 'enjoy', 'enjoying', 'episode', 'especially', 'essay', 'evening', 'exactly', 'exam', 'exams', 'excited', 'exhausted', 'expensive', 'eye', 'eyes', 'face', 'facebook', 'fact', 'fail', 'failed', 'fair', 'fall', 'falling', 'family', 'fan', 'far', 'fast', 'fat', 'favorite', 'fb', 'feel', 'feelin', 'feeling', 'feels', 'feet', 'fell', 'felt', 'fever', 'fight', 'figure', 'film', 'final', 'finally', 'finals', 'fine', 'finish', 'finished', 'fit', 'fix', 'fixed', 'flight', 'flu', 'fml', 'follow', 'followers', 'following', 'food', 'foot', 'forever', 'forget', 'forgot', 'forward', 'freaking', 'free', 'french', 'friday', 'friend', 'friends', 'fuck', 'fucking', 'fun', 'funny', 'gah', 'game', 'games', 'garden', 'gave', 'gay', 'gets', 'gettin', 'getting', 'girl', 'girls', 'giving', 'glad', 'god', 'goes', 'goin', 'going', 'gone', 'gonna', 'good', 'goodbye', 'goodnight', 'google', 'gosh', 'got', 'gotta', 'great', 'green', 'gt', 'guess', 'gutted', 'guy', 'guys', 'gym', 'ha', 'haha', 'hahaha', 'hair', 'half', 'hand', 'hands', 'hang', 'hanging', 'hangover', 'happen', 'happened', 'happens', 'happy', 'hard', 'hasn', 'hate', 'hates', 'haven', 'havent', 'having', 'head', 'headache', 'heading', 'hear', 'heard', 'heart', 'hell', 'hello', 'help', 'hey', 'hi', 'high', 'history', 'hit', 'hmm', 'hmmm', 'hold', 'holiday', 'holidays', 'home', 'homework', 'hope', 'hopefully', 'hoping', 'horrible', 'hospital', 'hot', 'hotel', 'hour', 'hours', 'house', 'hrs', 'http', 'hubby', 'hug', 'huge', 'hungry', 'hurt', 'hurting', 'hurts', 'ice', 'idea', 'idk', 'ill', 'im', 'inside', 'instead', 'interesting', 'internet', 'iphone', 'ipod', 'isn', 'isnt', 'itunes', 'ive', 'jealous', 'job', 'jonas', 'june', 'jus', 'just', 'keeps', 'kid', 'kids', 'kill', 'killed', 'killing', 'kind', 'kinda', 'knee', 'knew', 'know', 'knows', 'la', 'lady', 'lame', 'laptop', 'late', 'lately', 'later', 'laundry', 'laying', 'lazy', 'learn', 'leave', 'leaving', 'left', 'leg', 'legs', 'let', 'library', 'life', 'like', 'lil', 'line', 'link', 'list', 'listen', 'listening', 'little', 'live', 'living', 'll', 'lmao', 'load', 'loads', 'lol', 'london', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lose', 'losing', 'loss', 'lost', 'lot', 'lots', 'love', 'loved', 'lovely', 'loves', 'low', 'lt', 'luck', 'lucky', 'lunch', 'mac', 'mad', 'make', 'makes', 'making', 'man', 'math', 'matter', 'maybe', 'mcfly', 'mean', 'means', 'meant', 'meet', 'meeting', 'message', 'messed', 'middle', 'miles', 'mileycyrus', 'milk', 'min', 'mind', 'mins', 'minute', 'minutes', 'miss', 'missed', 'misses', 'missing', 'mobile', 'mom', 'moment', 'mommy', 'monday', 'money', 'month', 'months', 'mood', 'morning', 'mother', 'mothers', 'movie', 'movies', 'moving', 'mr', 'mum', 'music', 'myspace', 'nap', 'nd', 'near', 'nearly', 'neck', 'need', 'needed', 'needs', 'new', 'news', 'nice', 'night', 'nights', 'nite', 'nope', 'nose', 'number', 'office', 'officially', 'oh', 'ok', 'okay', 'old', 'omg', 'ones', 'online', 'open', 'ouch', 'outside', 'packing', 'page', 'pain', 'paper', 'parents', 'park', 'party', 'pass', 'passed', 'past', 'pay', 'pc', 'people', 'perfect', 'person', 'phone', 'photo', 'photos', 'pic', 'pick', 'pics', 'picture', 'pictures', 'pissed', 'pizza', 'place', 'plan', 'plans', 'play', 'played', 'playing', 'plus', 'pm', 'point', 'poor', 'post', 'power', 'ppl', 'pretty', 'probably', 'problem', 'problems', 'project', 'prom', 'ps', 'puppy', 'quiet', 'quite', 'quot', 'race', 'radio', 'rain', 'raining', 'rainy', 'ran', 'rd', 'read', 'reading', 'ready', 'real', 'realized', 'really', 'reason', 'red', 'remember', 'reply', 'rest', 'revising', 'revision', 'ride', 'right', 'rip', 'road', 'rock', 'room', 'round', 'ruined', 'run', 'running', 'sad', 'sadly', 'said', 'sat', 'saturday', 'save', 'saw', 'say', 'saying', 'says', 'scared', 'scary', 'school', 'screen', 'season', 'second', 'seeing', 'seen', 'send', 'sent', 'series', 'seriously', 'service', 'set', 'shall', 'shame', 'shift', 'shit', 'shoes', 'shoot', 'shop', 'shopping', 'short', 'shouldn', 'shower', 'shows', 'shut', 'sick', 'sigh', 'sign', 'single', 'sister', 'sit', 'site', 'sitting', 'sleep', 'sleeping', 'sleepy', 'slept', 'slow', 'small', 'snow', 'sold', 'son', 'song', 'songs', 'soo', 'soon', 'sooo', 'soooo', 'sooooo', 'sore', 'sorry', 'sound', 'sounds', 'spend', 'spending', 'spent', 'spring', 'st', 'stand', 'star', 'start', 'started', 'starting', 'starts', 'stay', 'stayed', 'staying', 'stomach', 'stop', 'stopped', 'store', 'story', 'stuck', 'study', 'studying', 'stuff', 'stupid', 'suck', 'sucks', 'summer', 'sun', 'sunday', 'sunny', 'sunshine', 'super', 'support', 'suppose', 'supposed', 'sure', 'sweet', 'swine', 'taken', 'taking', 'talk', 'talking', 'tea', 'team', 'teeth', 'tell', 'terrible', 'test', 'text', 'th', 'thank', 'thanks', 'thats', 'theres', 'thing', 'things', 'think', 'thinking', 'thinks', 'tho', 'thought', 'throat', 'thursday', 'ticket', 'tickets', 'til', 'till', 'tim', 'time', 'times', 'tired', 'today', 'told', 'tom', 'tommcfly', 'tomorrow', 'tonight', 'took', 'totally', 'tour', 'town', 'traffic', 'train', 'trek', 'tried', 'trip', 'trouble', 'true', 'try', 'trying', 'tuesday', 'tummy', 'turn', 'turned', 'tv', 'tweet', 'tweetdeck', 'tweeting', 'tweets', 'twitter', 'ugh', 'uk', 'understand', 'unfortunately', 'uni', 'update', 'updates', 'upload', 'upset', 'ur', 'use', 'used', 'using', 'usual', 'usually', 'vacation', 've', 'vegas', 'video', 'visit', 'voice', 'wait', 'waiting', 'wake', 'waking', 'walk', 'walking', 'wanna', 'want', 'wanted', 'wanting', 'wants', 'warm', 'wasn', 'wasnt', 'wat', 'watch', 'watched', 'watching', 'water', 'way', 'wear', 'wearing', 'weather', 'web', 'website', 'wedding', 'wednesday', 'week', 'weekend', 'weekends', 'weeks', 'weird', 'went', 'wet', 'whats', 'white', 'wife', 'win', 'window', 'windows', 'wish', 'wishes', 'wishing', 'wit', 'woke', 'wolverine', 'won', 'wonder', 'wondering', 'wont', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worried', 'worse', 'worst', 'worth', 'wouldn', 'wow', 'write', 'writing', 'wrong', 'wtf', 'xd', 'xx', 'xxx', 'ya', 'yay', 'yea', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yo', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "stem_column_list_ps = list(term_stem_ps_df.columns)\n",
    "print(stem_column_list_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(n_components=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(n_components=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.fit(term_stem_ps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10447792, 0.03239637, 0.02385348, ..., 0.21935678, 0.02402484,\n",
       "        0.02449801],\n",
       "       [0.02260185, 0.00618046, 0.02563076, ..., 0.06808363, 0.00829507,\n",
       "        0.01613169]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May need to use TextBlob? What does Tim Recommend.\n",
    "### Also need NLTK \n",
    "numbers and strings are an issue 00_00\n",
    "suggests we get rid of them altogether, use stop_words = english\n",
    "\n",
    "has to be 1% or more to show up in matrix\n",
    "min_df = .01\n",
    "TfidfVectorizer(stopwords = 'english', min_df = .01)\n",
    "any word is so basic you don't want to consider it. \"is\", \"the\", \"we\", \"are\". \"almost always better\"\n",
    "both work exactly the same\n",
    "\n",
    "you can also do it lower if you want to see uncommon\n",
    "\n",
    "do a random sample instead of the first 100,000.\n",
    "in pandas you can take a random assortment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets_100K_list[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I interpret these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "score = analyzer.polarity_scores(text)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will add up to 1, 28% negative, 72% neutral, . compound is normalized\n",
    "# leaning towards negative all over\n",
    "# models based on other people's opinions. doesn't know slang. \n",
    "# can be biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NMF is Tim's favorite, but try them all. works better with texts\n",
    "# & tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use df.sample(100_000) #easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
